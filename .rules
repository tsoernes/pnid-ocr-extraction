# P&ID OCR & Graph Extraction - Comprehensive Repository Summary

**Project**: P&ID OCR & Graph Extraction for Process Diagrams  
**Organization**: Bouvet ASA  
**Status**: Development (v0.1.0)  
**Last Updated**: 2025-12-11

---

## ğŸ“Š Executive Overview

This repository provides a complete toolkit for extracting structured information from Process & Instrumentation Diagrams (P&IDs) using multiple AI approaches:

- **Local OCR**: DeepSeek-OCR via Ollama with bounding box coordinates
- **Cloud AI**: Google Gemini, Azure Anthropic Claude, Azure DeepSeek
- **Visualization**: Interactive PyVis network graphs with diagram overlays
- **Data Models**: Pydantic models with spatial (x,y) coordinates

**Primary Use Case**: Convert brewery process diagrams into structured graph data (components + pipes) with spatial positioning.

**Technology Stack**:
- Python 3.12+ with modern packaging (`pyproject.toml`)
- Ollama for local LLM inference
- Pydantic AI for cloud model integration
- PyVis + NetworkX for interactive visualization
- Pillow for image processing

---

## ğŸ“ Repository Structure

```
pnid-ocr-extraction/
â”œâ”€â”€ src/                           # Core source code
â”‚   â”œâ”€â”€ ocr_bbox_overlay.py       # OCR bounding box parser & overlay
â”‚   â”œâ”€â”€ ollama_deepseel_ocr_fixed.py  # Ollama client for DeepSeek-OCR
â”‚   â”œâ”€â”€ run_overlay_demo.py       # OCR demo with bbox visualization
â”‚   â”œâ”€â”€ gemini_agent.py           # Google Gemini P&ID extraction
â”‚   â”œâ”€â”€ azure_antropic_agent.py   # Azure Anthropic Claude integration
â”‚   â”œâ”€â”€ azure_deepseek_agent.py   # Azure DeepSeek integration
â”‚   â”œâ”€â”€ plot_pnid_graph.py        # Interactive graph visualization
â”‚   â”œâ”€â”€ compare_bbox_scaling.py   # Bbox scaling comparison tool
â”‚   â”œâ”€â”€ debug_bbox.py             # Bbox coordinate debugger
â”‚   â”œâ”€â”€ ocr_with_bbox_demo.py     # Original demo (deprecated)
â”‚   â””â”€â”€ opencv_test.py            # Tesseract experiments (legacy)
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ README.md                 # Documentation index
â”‚   â”œâ”€â”€ STATUS_SUMMARY.md         # Current project status
â”‚   â”œâ”€â”€ WORKFLOW_AND_COMPARISON.md  # Complete workflows & model comparison
â”‚   â”œâ”€â”€ README_OCR_BoundingBox.md   # OCR technical guide
â”‚   â”œâ”€â”€ PROJECT_SETUP_COMPLETE.md   # Setup documentation
â”‚   â””â”€â”€ TRANSFER_TO_BOUVET.md     # Transfer checklist
â”‚
â”œâ”€â”€ data/                          # Data files
â”‚   â”œâ”€â”€ input/                    # Source diagrams
â”‚   â”‚   â”œâ”€â”€ brewary.png          # Primary test image (620Ã—345px)
â”‚   â”‚   â”œâ”€â”€ brewary.jpg
â”‚   â”‚   â””â”€â”€ brewary.svg
â”‚   â”œâ”€â”€ output/                   # Generated outputs
â”‚   â”‚   â”œâ”€â”€ pnid.json            # Extracted graph structure
â”‚   â”‚   â””â”€â”€ pnid_graph.html      # Interactive visualization
â”‚   â””â”€â”€ intermediate/             # Processing cache
â”‚
â”œâ”€â”€ examples/                      # Example outputs
â”‚   â”œâ”€â”€ brewary.json              # Sample P&ID extraction
â”‚   â””â”€â”€ gemini-brewery.svg        # Gemini output example
â”‚
â”œâ”€â”€ lib/                           # JavaScript libraries (for visualization)
â”‚   â”œâ”€â”€ vis-9.1.2/                # Vis.js network library
â”‚   â””â”€â”€ tom-select/               # Select component
â”‚
â”œâ”€â”€ tests/                         # Test files (currently empty)
â”‚
â”œâ”€â”€ .env                           # API keys (gitignored, copied from sibling)
â”œâ”€â”€ .gitignore                     # Git ignore patterns
â”œâ”€â”€ README.md                      # Main project README
â””â”€â”€ pyproject.toml                 # Modern Python packaging config

```

---

## ğŸ”‘ Core Components

### 1. OCR & Bounding Box System

**File**: `src/ocr_bbox_overlay.py` (11K)

**Purpose**: Parse DeepSeek-OCR responses and overlay visual bounding boxes on images.

**Key Features**:
- Parses DeepSeek format: `Text<|ref|>text<|/ref|><|det|>[[x1,y1,x2,y2]]<|/det|>`
- 1000-bin normalized coordinates â†’ pixel conversion
- Auto-scaling based on image dimensions
- Colored rectangle overlays with labels
- RGBA â†’ RGB conversion for JPEG output
- Statistics: item counts, average bbox sizes

**Class**: `OCRBoundingBoxOverlay`
- `parse_ocr_output(ocr_response)` - Extract text + bboxes
- `overlay_bounding_boxes(image_path, parsed_items, output_path)` - Draw annotations
- `process_ocr_and_overlay(...)` - Complete pipeline
- `get_statistics(parsed_items)` - Compute metrics

**Coordinate System**:
- Input: [0, 1000] normalized (resolution-independent)
- Output: [0, width] Ã— [0, height] pixels
- Scaling: `pixel_x = bbox_x Ã— (width / 1000)`

---

### 2. Local OCR via Ollama

**File**: `src/ollama_deepseel_ocr_fixed.py` (1.8K)

**Purpose**: Run DeepSeek-OCR model locally via Ollama server.

**Function**: `run_deepseek_ocr_via_ollama(image_data, prompt, image_path)`

**Architecture**:
- HTTP POST to `http://localhost:11434/api/generate`
- Model: `deepseek-ocr` (6.7 GB)
- Input: Base64-encoded image + grounding prompt
- Output: JSON with `response` field (text + bbox coordinates)

**Example Prompt**: `"<|grounding|>Convert the document to markdown"`

**Current Status**: âš ï¸ Blocked - Ollama 0.4.4 incompatible with DeepSeek-OCR
- Requires Ollama upgrade (needs sudo)
- Error: "Model not supported by your version"

**Hardware**:
- CPU: Intel Core i7-1365U (12 cores)
- RAM: 30.9 GiB (12 GB available for inference)
- GPU: Intel Iris Xe (integrated, no GPU acceleration)
- Performance: CPU-only inference (~2-5 seconds per image)

---

### 3. P&ID Graph Extraction (Cloud AI)

**Purpose**: Extract structured component and pipe data from diagrams using multimodal LLMs.

#### Data Models (with Spatial Coordinates)

```python
class Component(BaseModel):
    label: str          # e.g., "MAK", "MAT", "WOK"
    id: str             # Unique identifier
    category: str       # "Vessel", "Heat Exchanger", "Separator", etc.
    description: str    # Detailed description with parameters
    x: float            # Component center X coordinate (NEW)
    y: float            # Component center Y coordinate (NEW)

class Pipe(BaseModel):
    label: str          # Flow label
    source: str         # Source component ID
    target: str         # Target component ID
    description: str    # Stream properties (temp, composition)
    x: float            # Pipe label/midpoint X coordinate (NEW)
    y: float            # Pipe label/midpoint Y coordinate (NEW)

class PNID(BaseModel):
    components: list[Component]
    pipes: list[Pipe]
```

#### AI Agents

**Google Gemini** (`src/gemini_agent.py`)
- Model: `gemini-3-pro-preview`
- Provider: VertexAI with GoogleProvider
- Status: âš ï¸ Requires OAuth2 (API key not supported)
- Config: `GOOGLE_API_KEY` env var

**Azure Anthropic Claude** (`src/azure_antropic_agent.py`)
- Model: `claude-opus-4-5`
- Endpoint: `https://aif-minside.services.ai.azure.com/anthropic/`
- Status: âš ï¸ Requires API key
- Config: `AZURE_ANTROPIC_API_KEY` or `ANTHROPIC_FOUNDRY_API_KEY`

**Azure DeepSeek V3.1** (`src/azure_deepseek_agent.py`)
- Model: `DeepSeek-V3.1`
- Endpoint: `https://aif-minside.cognitiveservices.azure.com/`
- Status: âš ï¸ Requires API key
- Config: `AZURE_OPENAI_API_KEY`

---

### 4. Interactive Visualization

**File**: `src/plot_pnid_graph.py` (12K)

**Purpose**: Generate interactive HTML network graphs with P&ID diagram backgrounds.

**Function**: `create_interactive_graph(json_path, image_path, output_path)`

**Features**:
- PyVis Network with drag-and-drop nodes
- Background image (base64 embedded)
- Color-coded by category:
  - Vessels: Green (#4CAF50)
  - Heat Exchangers: Orange (#FF9800)
  - Separators: Blue (#2196F3)
  - Pumps: Purple (#9C27B0)
  - Valves: Red (#F44336)
  - Source: Gray (#9E9E9E)
  - Sink: Blue Gray (#607D8B)
- Hover tooltips with descriptions
- Physics simulation (toggleable)
- Background opacity control
- Node stabilization

**Output**: Standalone HTML file (`data/output/pnid_graph.html`)

**Current Status**: âœ… Working perfectly

---

## ğŸ”„ Workflows

### Workflow 1: Local OCR with Bounding Box Overlay

**Status**: âš ï¸ Blocked (Ollama version issue)

```bash
# 1. Start Ollama
ollama serve &

# 2. Run OCR + overlay demo
uv run src/run_overlay_demo.py

# Expected Output:
# - data/output/brewary_annotated.jpg
# - Console statistics
```

**Blocker**: Ollama 0.4.4 incompatible with DeepSeek-OCR

---

### Workflow 2: Cloud P&ID Extraction

**Status**: âš ï¸ Blocked (API keys needed)

```bash
# Configure .env file first
# Then run extraction:
uv run src/gemini_agent.py          # Google Gemini
uv run src/azure_antropic_agent.py  # Azure Claude
uv run src/azure_deepseek_agent.py  # Azure DeepSeek

# Expected Output:
# - data/output/pnid.json (with x,y coordinates)
```

**Blocker**: Missing API keys in `.env`

---

### Workflow 3: Interactive Visualization

**Status**: âœ… Working

```bash
# Generate interactive graph
uv run src/plot_pnid_graph.py

# Output: data/output/pnid_graph.html

# Open in browser
xdg-open data/output/pnid_graph.html
```

**Features**:
- Drag nodes to reposition
- Scroll to zoom
- Hover for details
- Toggle physics
- Adjust background opacity

---

## ğŸ“Š Model Comparison

| Feature | DeepSeek-OCR (Ollama) | Google Gemini | Azure Claude | Azure DeepSeek |
|---------|----------------------|---------------|--------------|----------------|
| **Status** | âš ï¸ Blocked (version) | âš ï¸ OAuth2 required | âš ï¸ API key needed | âš ï¸ API key needed |
| **Deployment** | Local (CPU-only) | Cloud | Cloud | Cloud |
| **Cost** | Free | Pay-per-token | Pay-per-token | Pay-per-token |
| **Latency** | 2-5s (CPU) | <2s | <2s | <1s |
| **Privacy** | Full (local) | Cloud | Cloud | Cloud |
| **Bounding Boxes** | âœ… Yes (1000-bin) | âŒ No | âŒ No | âŒ No |
| **Structured Output** | âŒ Manual parsing | âœ… Pydantic | âœ… Pydantic | âœ… Pydantic |
| **Spatial Coords** | âœ… Via bbox | âœ… x,y fields | âœ… x,y fields | âœ… x,y fields |
| **Model Size** | 6.7 GB | N/A | N/A | N/A |
| **Offline** | âœ… Yes | âŒ No | âŒ No | âŒ No |

**Unique to DeepSeek-OCR**: Bounding box coordinates with 1000-bin normalization

**Best for**:
- **Quality**: Azure Anthropic Claude Opus 4.5
- **Cost**: Azure DeepSeek V3.1
- **Privacy**: Local Ollama DeepSeek-OCR
- **Bounding Boxes**: DeepSeek-OCR only

---

## ğŸ› ï¸ Technical Details

### Coordinate Systems

**DeepSeek-OCR (1000-bin normalized)**:
- Range: [0, 1000] for both x and y
- Resolution-independent
- Scaling formula:
  ```python
  scale_x = image_width / 1000.0
  scale_y = image_height / 1000.0
  pixel_x = bbox_x * scale_x
  pixel_y = bbox_y * scale_y
  ```

**Cloud Models (pixel coordinates)**:
- Range: [0, image_width] for x, [0, image_height] for y
- Direct pixel positions
- Image-specific (not normalized)

### Example Data

**Current Output** (`data/output/pnid.json`):
- 14 components (vessels, heat exchangers, separators)
- 33 pipes (process streams with temperatures)
- Missing: x,y coordinates (old model)

**Components**:
- MAK (Mashing vessel for Malt, Corn, Water)
- MAT (Mashing vessel for Malt, Water)
- WOK (Wort kettle/Whirlpool)
- Heat Exchangers (steam heaters, main cooler)
- Filter, Centrifuge
- Hot Water Tank (HWT)

**Pipes**:
- Temperature ranges: 10Â°C to 102Â°C
- Stream types: Malt, Corn, Water, Steam, Wort
- Waste outputs: Dreche (solids), Trub, Gas

---

## ğŸ“¦ Dependencies

**Defined in**: `pyproject.toml`

### Core Dependencies
```toml
dependencies = [
    "Pillow>=9.0.0",        # Image processing
    "requests>=2.25.0",     # HTTP client
    "pydantic-ai",          # AI agent framework
    "python-dotenv",        # Environment variables
    "pyvis>=0.3.0",         # Interactive visualization
    "networkx>=3.0",        # Graph structures
    "anthropic",            # Anthropic API client
]
```

### Development Dependencies
```toml
[dependency-groups]
dev = [
    "pytest>=7.0.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
]
```

### Build System
- **Backend**: hatchling
- **Package Layout**: `src/` directory
- **Python**: >=3.12 (tested on 3.12.7 and 3.13.9)

### Installation
```bash
# Recommended (editable install)
uv pip install -e .

# With dev dependencies
uv pip install -e ".[dev]"

# Verify
python -c "from src.plot_pnid_graph import create_interactive_graph; print('âœ… OK')"
```

---

## âš™ï¸ Configuration

### Environment Variables (`.env`)

**Required for Cloud Models**:
```bash
# Google Gemini (requires OAuth2)
GOOGLE_API_KEY=your_key_here

# Azure Anthropic
AZURE_ANTROPIC_API_KEY=your_key_here
ANTHROPIC_FOUNDRY_API_KEY=your_key_here

# Azure DeepSeek
AZURE_OPENAI_API_KEY=your_key_here
```

**Note**: `.env` file is gitignored and copied from sibling `p&id/` directory

### Ollama Configuration

**Server**: `http://localhost:11434`
- Start: `ollama serve &`
- Model: `ollama pull deepseek-ocr`
- Status: `ollama list`

**Model Details**:
- Name: `deepseek-ocr:latest`
- Size: 6.7 GB
- Downloaded: âœ… Yes
- Compatible: âš ï¸ Needs Ollama upgrade

---

## âœ… Current Status

### Working Components
- âœ… Data models updated with x,y coordinates
- âœ… Interactive visualization (PyVis)
- âœ… All Python dependencies installed
- âœ… DeepSeek-OCR model downloaded
- âœ… Modern packaging with `pyproject.toml`
- âœ… Comprehensive documentation in `docs/`

### Blocked Components
- âš ï¸ Local OCR (Ollama version incompatible)
- âš ï¸ Google Gemini (OAuth2 required)
- âš ï¸ Azure Anthropic (API key needed)
- âš ï¸ Azure DeepSeek (API key needed)

### Recent Changes (Git History)
```
4b469e7 Copy .env, remove requirements.txt, delete src/data folder
f68ddd2 Migrate from requirements.txt to pyproject.toml with modern packaging
a46c4e2 Organize documentation into docs folder with index
4e9e36d Add executive status summary
c41689f Complete workflow guide, fix plot paths, remove unused imports
efce806 Add x,y coordinates to Component and Pipe data models
```

---

## ğŸš€ Quick Start

### Prerequisites
```bash
# System requirements
- Python 3.12+
- Ollama (for local OCR)
- UV (recommended for package management)

# Install Ollama (Fedora)
sudo dnf install ollama

# Install project
git clone <repo-url>
cd pnid-ocr-extraction
uv pip install -e .
```

### Run Visualization (Works Now)
```bash
uv run src/plot_pnid_graph.py
xdg-open data/output/pnid_graph.html
```

### Run Cloud Extraction (Needs API Keys)
```bash
# 1. Configure .env with API keys
# 2. Run extraction
uv run src/azure_antropic_agent.py

# 3. Verify output
cat data/output/pnid.json | jq '.components[0]'
```

### Run Local OCR (Blocked - Needs Ollama Upgrade)
```bash
# Will fail until Ollama is upgraded
uv run src/run_overlay_demo.py
```

---

## ğŸ” Key Files

### Source Code (`src/`)
- `ocr_bbox_overlay.py` - OCR parser & overlay (11K, core functionality)
- `plot_pnid_graph.py` - Visualization (12K, working)
- `gemini_agent.py` - P&ID extraction with Pydantic models
- `ollama_deepseel_ocr_fixed.py` - Ollama client
- `run_overlay_demo.py` - OCR demo script

### Documentation (`docs/`)
- `STATUS_SUMMARY.md` - Executive status (301 lines)
- `WORKFLOW_AND_COMPARISON.md` - Complete guide (627 lines)
- `README_OCR_BoundingBox.md` - OCR technical details

### Configuration
- `pyproject.toml` - Modern Python packaging (125 lines)
- `.env` - API keys (gitignored)
- `.gitignore` - Ignore patterns

### Data
- `data/input/brewary.png` - Test diagram (620Ã—345px)
- `data/output/pnid.json` - Extracted graph (14 components, 33 pipes)
- `data/output/pnid_graph.html` - Interactive visualization

---

## ğŸ“ Development Notes

### Code Quality Tools (Configured)
- **Black**: Line length 100, Python 3.12+
- **Ruff**: Pycodestyle, pyflakes, isort, bugbear
- **MyPy**: Strict type checking (disabled for external libs)
- **Pytest**: Test framework (tests/ currently empty)

### Git Workflow
- Main branch: `main`
- Recent commits focused on:
  - Modern packaging migration
  - Documentation organization
  - Data model updates (spatial coordinates)
  - Path fixes and code cleanup

### Known Issues
1. Ollama 0.4.4 incompatible with DeepSeek-OCR (needs upgrade)
2. Cloud models require API key configuration
3. Gemini requires OAuth2 (complex setup)
4. No test coverage yet (`tests/` empty)

---

## ğŸ¯ Next Steps

### Immediate (Unblocked by API Keys)
1. Configure Azure Anthropic API key in `.env`
2. Run P&ID extraction with x,y coordinates
3. Verify spatial coordinate accuracy
4. Update visualization to use real positions (not random)

### Medium-term (Requires Ollama Upgrade)
1. Upgrade Ollama to latest version
2. Test local OCR with bounding boxes
3. Compare bbox coordinates vs. LLM spatial extraction
4. Benchmark all models (accuracy, speed, cost)

### Long-term
1. Add test coverage (pytest)
2. Implement coordinate validation
3. Batch processing for multiple diagrams
4. Export to CAD formats (DXF, GraphML)
5. CI/CD pipeline with GitHub Actions

---

## ğŸ“š Documentation Index

All documentation is in `docs/`:

- **[docs/README.md](docs/README.md)** - Documentation index
- **[docs/STATUS_SUMMARY.md](docs/STATUS_SUMMARY.md)** - Current status & next steps
- **[docs/WORKFLOW_AND_COMPARISON.md](docs/WORKFLOW_AND_COMPARISON.md)** - Complete workflows
- **[docs/README_OCR_BoundingBox.md](docs/README_OCR_BoundingBox.md)** - OCR technical guide

**Main README**: [README.md](README.md)

---

## ğŸ¢ Project Information

**Organization**: Bouvet ASA  
**Repository**: pnid-ocr-extraction  
**Version**: 0.1.0 (Development)  
**License**: Proprietary  
**Python**: >=3.12  
**Build System**: hatchling  

**Keywords**: ocr, pnid, process-diagram, graph-extraction, ai, deepseek, ollama

**URLs**:
- Homepage: https://github.com/bouvet/pnid-ocr-extraction
- Documentation: https://github.com/bouvet/pnid-ocr-extraction/tree/main/docs
- Issues: https://github.com/bouvet/pnid-ocr-extraction/issues

---

**Last Summary Generated**: 2025-12-11  
**Summary Method**: Manual comprehensive documentation  
**Total Files**: 23 (Python, Markdown, JSON, SVG, TOML)  
**Lines of Code**: ~2,500 (Python), ~1,800 (Documentation)